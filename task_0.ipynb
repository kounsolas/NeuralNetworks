{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "043520cc-ceb0-4492-97ea-857e3d22f0af",
   "metadata": {},
   "source": [
    "# Report exercise 0\n",
    "\n",
    "In this notebook i will be comparing the performance of the 1NN, 3NN and Nearest Centroid classifier, using the `CIFAR-10` dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3c449b-5bf5-47aa-b741-92e473c932b0",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "- I am importing the `numpy` library because the `unpickle()` function returns numpy arrays and i will need to manipulate them\n",
    "\n",
    "- I used the recommended function `unpickle(file)` that i saw in this site https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "\n",
    "- In this cell, i am storing the batch names in an array called `file_names` and i am also using an array called `files` to store the 6 dictionaries that i will get from the `unpickle()` function.\n",
    "\n",
    "- At the end i am printing the number of the elements that are in `files`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8079e5f4-6dd6-44ee-afa0-534e654693ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"LOKY_MAX_CPU_COUNT\"] = \"8\"  # This line erases a warning that i am getting\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "file_names = []\n",
    "files = []\n",
    "\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "\n",
    "for i in range(1,6):\n",
    "    file = f\"data_batch_{i}\"\n",
    "    file_names.append(file)\n",
    "\n",
    "file_names.append(\"test_batch\")\n",
    "\n",
    "for file in file_names:\n",
    "    cifar10_dict = unpickle(file)\n",
    "    files.append(cifar10_dict)\n",
    "\n",
    "n_of_files = len(file_names)\n",
    "n_of_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ee043b-da31-4147-a92b-7d37fe916073",
   "metadata": {},
   "source": [
    "## Inspect the list `files`\n",
    "* In this list i have 6 dictionaries\n",
    "* Below i get to see the keys from all the dictionaries\n",
    "* I notice that i have 5 training batches, each one with 10000 images and 1 testing batch with 10000 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72a51ec3-6f78-4e4a-8f7e-51c69eaa1973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary data_batch_1: \n",
      "Keys: dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n",
      "Batch label: b'training batch 1 of 5'\n",
      "Data size: (10000, 3072) \n",
      "\n",
      "Dictionary data_batch_2: \n",
      "Keys: dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n",
      "Batch label: b'training batch 2 of 5'\n",
      "Data size: (10000, 3072) \n",
      "\n",
      "Dictionary data_batch_3: \n",
      "Keys: dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n",
      "Batch label: b'training batch 3 of 5'\n",
      "Data size: (10000, 3072) \n",
      "\n",
      "Dictionary data_batch_4: \n",
      "Keys: dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n",
      "Batch label: b'training batch 4 of 5'\n",
      "Data size: (10000, 3072) \n",
      "\n",
      "Dictionary data_batch_5: \n",
      "Keys: dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n",
      "Batch label: b'training batch 5 of 5'\n",
      "Data size: (10000, 3072) \n",
      "\n",
      "Dictionary test_batch: \n",
      "Keys: dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n",
      "Batch label: b'testing batch 1 of 1'\n",
      "Data size: (10000, 3072) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_of_files):\n",
    "    print(f\"Dictionary {file_names[i]}: \\nKeys: {files[i].keys()}\\nBatch label: {files[i][b'batch_label']}\\nData size: {files[i][b'data'].shape} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb4dacb-03d8-4d81-ae79-17e79dc3736d",
   "metadata": {},
   "source": [
    "## Since i have the contents of the cifar10, i am keeping only the data and the labels\n",
    "\n",
    "- I am printing the name and the keys of each one of the 6 dictionaries, in order to be sure, that i am only keeping the data and the labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc00234f-7959-4681-905c-b6beb3942fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary data_batch_1: \n",
      "Keys: dict_keys([b'labels', b'data'])\n",
      "\n",
      "Dictionary data_batch_2: \n",
      "Keys: dict_keys([b'labels', b'data'])\n",
      "\n",
      "Dictionary data_batch_3: \n",
      "Keys: dict_keys([b'labels', b'data'])\n",
      "\n",
      "Dictionary data_batch_4: \n",
      "Keys: dict_keys([b'labels', b'data'])\n",
      "\n",
      "Dictionary data_batch_5: \n",
      "Keys: dict_keys([b'labels', b'data'])\n",
      "\n",
      "Dictionary test_batch: \n",
      "Keys: dict_keys([b'labels', b'data'])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_of_files):\n",
    "    #use try except because if i try to run this cell alone, an error will occur, because i have already deleted these keys\n",
    "    try:\n",
    "        del files[i][b'batch_label']\n",
    "        del files[i][b'filenames']\n",
    "    except KeyError:\n",
    "        print(\"These keys have already been deleted!\\nRun all the cells again\")\n",
    "        pass\n",
    "    print(f\"Dictionary {file_names[i]}: \\nKeys: {files[i].keys()}\\n\")\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61e9cf9-102b-47ec-8819-20841f05e802",
   "metadata": {},
   "source": [
    "## Create the X_train, y_train, X_test, y_test\n",
    "\n",
    "* At first, i initialize with zeroes 4 arrays.\n",
    "* Then the `X_train` and the `y_train` are filled. The first with 50000*3072 data (10000 images from each batch file) and the second with  50000 labels (10000 from each batch file).\n",
    "* When i get to the testing batch, indexed at the 5th place in the `files` array, i fill the `X_test` with 10000*3072 data (10000 images) and the `y_test` with 10000 labels\n",
    "* I am printing the size of each array, to showcase that the sizes are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "449cf8b3-c146-4f65-aa57-add9156f8846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape X_train: (50000, 3072)\n",
      "Shape y_train: (50000,)\n",
      "Shape X_test: (10000, 3072)\n",
      "Shape y_test: (10000,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create X_train, y_train, X_test, y_test\n",
    "X_train = np.full((50000,3072),0,dtype=int)\n",
    "X_test = np.full((10000,3072),0,dtype=int)\n",
    "y_train = np.full((50000,),0,dtype=int)\n",
    "y_test = np.full((10000,),0,dtype=int)\n",
    "\n",
    "for i in range(n_of_files):\n",
    "    if i != 5:\n",
    "        #this is the X_train, y_train\n",
    "        X_train[i*10000:(i+1)*10000,:] = files[i][b'data']\n",
    "        y_train[i*10000:(i+1)*10000] = files[i][b'labels']\n",
    "    else:\n",
    "        #i have just finished X_train, y_train\n",
    "        print(f\"Shape X_train: {X_train.shape}\\nShape y_train: {y_train.shape}\")\n",
    "        #this is the X_test, y_test\n",
    "        X_test[:,:] = files[i][b'data']\n",
    "        y_test[:] = files[i][b'labels']\n",
    "        print(f\"Shape X_test: {X_test.shape}\\nShape y_test: {y_test.shape}\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9058a930-7390-47e6-88f0-9bc77e20d3a5",
   "metadata": {},
   "source": [
    "## Preprocessing the data\n",
    "\n",
    "In the cell bellow, i will try some methods to improve the accuracy of my algorithms by preprocessing the data\n",
    "\n",
    "- `fit_transform` is used on `X_train` to calculate the mean and standard deviation of the training data (this is the \"fit\" part).\n",
    "It then scales `X_train` using those calculated values (this is the \"transform\" part).\n",
    "The `transform` only applies the previously computed mean and standard deviation (from X_train) to scale X_test. This way i am getting the `X_train_sc` and `X_test_sc`\n",
    "\n",
    "- Using the PCA, the dimensionality of the dataset is reduced to 100 features (these are the features with the most variance), by excluding the features with the least variance. This way i am getting the `X_train_PCA` and `X_test_PCA`\n",
    "\n",
    "- Adding some guassian noise in the training data improves the accuracy and the robustness of our model. In more detail, the noise that i am adding has mean = 0 and varaince = 2. With bigger values, i am getting worse accuracy. I am getting `X_train_sc_PCA_noisy`\n",
    "\n",
    "- So right now i have:\n",
    "    * the initial data\n",
    "    * the scaled data\n",
    "    * the PCA transformed data\n",
    "    * the scaled PCA transformed noisy data \n",
    "  \n",
    "This way i will monitor how the accuracy of the algorithms improve with these changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "558e39c7-fc67-49e9-a6e8-84ea2fd58ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Feature scaling\n",
    "sc_X = StandardScaler()\n",
    "X_train_sc = sc_X.fit_transform(X_train)\n",
    "X_test_sc = sc_X.transform(X_test)\n",
    "\n",
    "\n",
    "#PCA\n",
    "pca = PCA(n_components=100)\n",
    "X_train_PCA = pca.fit_transform(X_train)\n",
    "X_test_PCA = pca.transform(X_test)\n",
    "\n",
    "X_train_sc_PCA = pca.fit_transform(X_train_sc)\n",
    "X_test_sc_PCA = pca.transform(X_test_sc)\n",
    "\n",
    "#adding gaussian noise to the dataset\n",
    "noise_factor = 0.1  # Adjust the noise level as needed\n",
    "X_train_sc_PCA_noisy = X_train_sc_PCA + noise_factor * np.random.normal(loc=0.0, scale=2.0, size=X_train_sc_PCA.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6235f520-b718-47a7-89f7-1a04cc60cb4e",
   "metadata": {},
   "source": [
    "## Impement KNN\n",
    "\n",
    "* So the concept is this:\n",
    "``\n",
    "I am working in the 3072 dimensional space and i have seen all the training batches.\n",
    "The new image will use the KNN to find the K nearest images, using a defined metric (Euklideian Distance,cosine,...)\n",
    "Then the majority class (label) between the K nearest images will be the class (label) of the new image \n",
    "``"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa9f396b-3540-4f01-a083-606615057960",
   "metadata": {},
   "source": [
    "## Implement 1NN\n",
    "\n",
    "* I noticed that by using the `weight = \"distance\"` and `metric = \"cosine\"` the accuracy improves. I noticed that by trying all the `weights` and the `metrics` available\n",
    "\n",
    "\n",
    "I have tried many different things:\n",
    "* used all the other metrics: `cityblock`, `haversine`, `l1`, `l2`, `manhattan`, `nan_euclidean` < 0.35.\n",
    "\n",
    "* Also i used `weights = \"distance\"` and i got a slightly better accuracy. This means that neighbors that are nearer to the query point will have a greater influence on the predicted class\n",
    "The default value is `weights = \"uniform\"` (each neighbor contributes equally to the decision.)\n",
    "\n",
    "* I used also the initial data, then the scaled data, then the PCA transformed data ,then the scaled and PCA transformed data and the scaled,PCA,noisy data\n",
    "\n",
    "Let's showcase some of the tries below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d370af38-b37b-4502-8288-a1f4cd833fb0",
   "metadata": {},
   "source": [
    "### 1NN using:\n",
    "\n",
    "* the initial data \n",
    "* weights = `uniform`\n",
    "* metric = `euclidean`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be8f80f3-1851-41a6-a2c0-a1acb20b7880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 1-NN: 0.3539\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the model: init 1-NN\n",
    "classifier_1NN = KNeighborsClassifier(n_neighbors=1,weights=\"uniform\",metric=\"euclidean\")  #i noticed that with the cosine metric, the accuracy is higher than with the euclidean\n",
    "\n",
    "# Train the model\n",
    "# Only the training batches\n",
    "classifier_1NN.fit(X_train,y_train)\n",
    "\n",
    "# predict the labels from the test batch data\n",
    "y_pred_labels = classifier_1NN.predict(X_test)\n",
    "\n",
    "# Evaluate the model using accuracy (y_pred_labels == y_test) / number of tests\n",
    "# number of tests = 10k\n",
    "print(f\"Accuracy of 1-NN: {accuracy_score(y_test,y_pred_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c549e3a-e621-4ff9-865b-fae361cb1887",
   "metadata": {},
   "source": [
    "### 1NN using\n",
    "\n",
    "* scaled data\n",
    "* metric = `cosine`\n",
    "* weights = `distance`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3d1f058-0f17-4e94-a0df-35dc540c20ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 1-NN (2): 0.4102\n"
     ]
    }
   ],
   "source": [
    "classifier_1NN = KNeighborsClassifier(n_neighbors=1,weights=\"distance\",metric=\"cosine\")\n",
    "classifier_1NN.fit(X_train_sc,y_train)\n",
    "y_pred_labels = classifier_1NN.predict(X_test_sc)\n",
    "print(f\"Accuracy of 1-NN (2): {accuracy_score(y_test,y_pred_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e013fa44-f22d-4c44-ae46-6adacf73259a",
   "metadata": {},
   "source": [
    "### 1NN using:\n",
    "\n",
    "* PCA transformed data\n",
    "* weights = `distance`\n",
    "* metric = `cosine`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09f6003d-fc7b-4e36-8c50-6d6b1387b100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 1-NN (3): 0.4221\n"
     ]
    }
   ],
   "source": [
    "classifier_1NN.fit(X_train_PCA,y_train)\n",
    "y_pred_labels = classifier_1NN.predict(X_test_PCA)\n",
    "print(f\"Accuracy of 1-NN (3): {accuracy_score(y_test,y_pred_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0624f2e-3831-41e4-94d9-0020161e2db2",
   "metadata": {},
   "source": [
    "### 1NN using:\n",
    "\n",
    "* scaled and PCA transformed data\n",
    "* weights = `distance`  \n",
    "* metric = `cosine`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6873180-7813-44c4-a8db-4994c1d84ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 1-NN (4): 0.4244\n"
     ]
    }
   ],
   "source": [
    "classifier_1NN.fit(X_train_sc_PCA,y_train)\n",
    "y_pred_labels = classifier_1NN.predict(X_test_sc_PCA)\n",
    "print(f\"Accuracy of 1-NN (4): {accuracy_score(y_test,y_pred_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a66916e-c6d5-4f44-a401-e2381a296866",
   "metadata": {},
   "source": [
    "### 1NN using:\n",
    "\n",
    "* scaled, PCA transformed and noisy data\n",
    "* weights = `distance`\n",
    "* metric = `cosine`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f88a1e43-a246-4e30-8b5f-722c907d2412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 1-NN (5): 0.4247\n"
     ]
    }
   ],
   "source": [
    "classifier_1NN.fit(X_train_sc_PCA_noisy,y_train)\n",
    "y_pred_labels = classifier_1NN.predict(X_test_sc_PCA)\n",
    "print(f\"Accuracy of 1-NN (5): {accuracy_score(y_test,y_pred_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5c692a-0eca-44c7-ab93-0a77a939952e",
   "metadata": {},
   "source": [
    "Now lets try the above cell but with weights = `uniform` and metrics = `euclidean` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8029c11-f15a-4e82-8762-f9a928956f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 1-NN (6): 0.3844\n"
     ]
    }
   ],
   "source": [
    "classifier_1NN = KNeighborsClassifier(n_neighbors=1,weights=\"uniform\",metric=\"euclidean\")\n",
    "classifier_1NN.fit(X_train_sc_PCA_noisy,y_train)\n",
    "y_pred_labels = classifier_1NN.predict(X_test_sc_PCA)\n",
    "print(f\"Accuracy of 1-NN (6): {accuracy_score(y_test,y_pred_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e674e3fb-269a-452b-8840-dde12c25c6f6",
   "metadata": {},
   "source": [
    "### Conclusion for 1NN\n",
    "\n",
    "So i come to the conclusion that the best performance that i achieved for 1NN was 0.4247. I achieved this by having scaled, PCA transformed noisy data.\n",
    "Also i tuned the parameters of the `KNeighborsclassifier` (`metrics` and `distance`).\n",
    "I should also say that in some of the testings that i did, the best accuracy was achieved for the scaled and PCA transformed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b0979b-f2d0-4657-8a5f-f44a71701e20",
   "metadata": {},
   "source": [
    "## Implement the 3NN\n",
    "\n",
    "Lets also try the same testings that i did above for the 3NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a299953-53e1-44b9-81fe-ee41ef04467b",
   "metadata": {},
   "source": [
    "### 3NN using:\n",
    "\n",
    "* initial data\n",
    "* weights = `uniform`\n",
    "* metric = `euclidean`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06ada456-5286-4595-a6a6-7695c1e98f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 3-NN: 0.3303\n"
     ]
    }
   ],
   "source": [
    "# Define the model: init 3-NN\n",
    "classifier_3NN = KNeighborsClassifier(n_neighbors=3,weights=\"uniform\",metric=\"euclidean\")  #i noticed that with the cosine metric, the accuracy is higher than with the euclidean\n",
    "\n",
    "# Train the model\n",
    "# Only the training batches\n",
    "classifier_3NN.fit(X_train,y_train)\n",
    "\n",
    "# Predict the test set results\n",
    "# predict the labels from the test batch data\n",
    "y_pred_labels = classifier_3NN.predict(X_test)\n",
    "\n",
    "# Evaluate the model using accuracy (y_pred_labels == y_test_labels) / number of tests\n",
    "# number of tests = 10k\n",
    "print(f\"Accuracy of 3-NN: {accuracy_score(y_test,y_pred_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e1ce91-91f7-4b3e-a9e8-afa10c9a5ecc",
   "metadata": {},
   "source": [
    "### 3NN using:\n",
    "\n",
    "* initial data\n",
    "* weights = `distance`\n",
    "* metric = `cosine` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae69de79-f785-4d3d-982c-137c870d9622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 3-NN (2): 0.3813\n"
     ]
    }
   ],
   "source": [
    "classifier_3NN = KNeighborsClassifier(n_neighbors=3,weights=\"distance\",metric=\"cosine\")  \n",
    "classifier_3NN.fit(X_train,y_train)\n",
    "y_pred_labels = classifier_3NN.predict(X_test)\n",
    "print(f\"Accuracy of 3-NN (2): {accuracy_score(y_test,y_pred_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a47616f-f435-4fa6-a1d2-bcaa41ac8e77",
   "metadata": {},
   "source": [
    "### 3NN using:\n",
    "\n",
    "* scaled data\n",
    "* same weights and metric as the above cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ecc28235-416f-4e19-af83-47ee0607934a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 3-NN (3): 0.4272\n"
     ]
    }
   ],
   "source": [
    "classifier_3NN.fit(X_train_sc,y_train)\n",
    "y_pred_labels = classifier_3NN.predict(X_test_sc)\n",
    "print(f\"Accuracy of 3-NN (3): {accuracy_score(y_test,y_pred_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20baa60b-a082-4ef8-968d-d83fda8aceb3",
   "metadata": {},
   "source": [
    "### 3NN using:\n",
    "\n",
    "* PCA transformed data\n",
    "* same weights and metric as the above cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c26ad66-548a-4892-99fc-3652ee16fb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 3-NN (4): 0.4422\n"
     ]
    }
   ],
   "source": [
    "classifier_3NN.fit(X_train_PCA,y_train)\n",
    "y_pred_labels = classifier_3NN.predict(X_test_PCA)\n",
    "print(f\"Accuracy of 3-NN (4): {accuracy_score(y_test,y_pred_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b82089-527b-436b-81ca-7bfc383e0835",
   "metadata": {},
   "source": [
    "### 3NN using:\n",
    "\n",
    "* scaled and PCA transformed data\n",
    "* same weights and metric as the above cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b839796b-dea6-4d15-9280-cf81e77210b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 3-NN (5): 0.4412\n"
     ]
    }
   ],
   "source": [
    "classifier_3NN.fit(X_train_sc_PCA,y_train)\n",
    "y_pred_labels = classifier_3NN.predict(X_test_sc_PCA)\n",
    "print(f\"Accuracy of 3-NN (5): {accuracy_score(y_test,y_pred_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd5a540-18dc-48e8-b19e-2e66c77c1b3d",
   "metadata": {},
   "source": [
    "### 3NN using:\n",
    "\n",
    "* scaled, PCA transformed and noisy data\n",
    "* same weights and metric as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3de582f-c26f-45bc-9531-0b66738ff5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 3-NN (6): 0.4425\n"
     ]
    }
   ],
   "source": [
    "classifier_3NN.fit(X_train_sc_PCA_noisy,y_train)\n",
    "y_pred_labels = classifier_3NN.predict(X_test_sc_PCA)\n",
    "print(f\"Accuracy of 3-NN (6): {accuracy_score(y_test,y_pred_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1920f094-9c27-4c47-ae3d-c41c7009e760",
   "metadata": {},
   "source": [
    "### Conclusion for 3NN\n",
    "\n",
    "So i see that the best performance that i achieved for 3NN was 0.4425. I achieved this having scaled, PCA transformed noisy data.\n",
    "Also i tuned the parameters of the `KNeighborsclassifier` (`metrics = cosine` and `weights = distance`).\n",
    "In some other testings that i did, the best accuracy was given by the PCA transformed data or the scaled PCA transformed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e29bd4f-2c65-40ea-af1c-f03742452c8c",
   "metadata": {},
   "source": [
    "## Implement of Nearest Centroid classifier\n",
    "\n",
    "* The concept of the NC is the following:\n",
    "  - For each class in the training data, compute the centroid (average of all feature points).\n",
    "  - For a new data point, calculate the distance to each class centroid.\n",
    "  - The label of the nearest centroid is assigned as the new data point's predicted class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec61551-161a-451a-80ee-a58d9babd240",
   "metadata": {},
   "source": [
    "### NC using:\n",
    "\n",
    "* initial data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71cfc6b5-aeca-47d2-8888-e3f7ec037a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of NC : 0.2737\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "classifier_KNC = NearestCentroid(shrink_threshold=1.4)\n",
    "\n",
    "#training the classifier\n",
    "classifier_KNC.fit(X_train,y_train)\n",
    "\n",
    "y_pred_labels = classifier_KNC.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy of NC : {accuracy_score(y_test,y_pred_labels)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfb7785-cb63-4dd0-98e7-cec86d45a9ac",
   "metadata": {},
   "source": [
    "### NC using:\n",
    "* scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45e6e77a-25a7-4764-a9d1-b79c9efa38fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of NC (2): 0.2781\n"
     ]
    }
   ],
   "source": [
    "classifier_KNC.fit(X_train_sc,y_train)\n",
    "y_pred_labels = classifier_KNC.predict(X_test_sc)\n",
    "print(f\"Accuracy of NC (2): {accuracy_score(y_test,y_pred_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09d2802-1111-4e1f-9fe2-66c1a4ec2455",
   "metadata": {},
   "source": [
    "### NC using:\n",
    "* PCA transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "430efc92-0401-4da3-b1d6-910b10b9c0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of NC (3) : 0.2723\n"
     ]
    }
   ],
   "source": [
    "classifier_KNC.fit(X_train_PCA,y_train)\n",
    "y_pred_labels = classifier_KNC.predict(X_test_PCA)\n",
    "print(f\"Accuracy of NC (3) : {accuracy_score(y_test,y_pred_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84028014-b39a-4276-9566-b98dd2ed0ae9",
   "metadata": {},
   "source": [
    "### NC using:\n",
    "* Scaled and PCA transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6aa3c52-a2b5-4c3f-8714-d2f347c5f79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of NC (4): 0.276\n"
     ]
    }
   ],
   "source": [
    "classifier_KNC.fit(X_train_sc_PCA,y_train)\n",
    "y_pred_labels = classifier_KNC.predict(X_test_sc_PCA)\n",
    "print(f\"Accuracy of NC (4): {accuracy_score(y_test,y_pred_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4737f48-a3db-4a09-b22b-d29124440dc0",
   "metadata": {},
   "source": [
    "### NC using:\n",
    "* Scaled, PCA transformed and noisy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43096d9a-1ac3-4c69-a060-7fb775a2963c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of NC (5): 0.2762\n"
     ]
    }
   ],
   "source": [
    "classifier_KNC.fit(X_train_sc_PCA_noisy,y_train)\n",
    "y_pred_labels = classifier_KNC.predict(X_test_sc_PCA)\n",
    "print(f\"Accuracy of NC (5): {accuracy_score(y_test,y_pred_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0688a7a8-aa59-4f08-9e16-c68fb4a5f703",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Conclusion for NC\n",
    "\n",
    "So i see that the best performance that i achieved for NC was 0.2781. I achieved this having only scaled the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a213d04e-72e2-4801-9bd2-9253be4b69de",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Having seen all the above, i can say that both the KNN (k=1 or k=3) and the NC are not good classifiers for the classification of the images from the CIFAR-10 dataset. I thought and used many differents methods to improve the accuracy of the algorithms, but the improvement wasn't significant.  \n",
    "\n",
    "```\n",
    "ΧΡΗΣΤΟΣ ΚΟΥΝΣΟΛΑΣ\n",
    "AEM : 10345\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
