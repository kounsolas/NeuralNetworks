{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac3c449b-5bf5-47aa-b741-92e473c932b0",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "8079e5f4-6dd6-44ee-afa0-534e654693ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "file_names = []\n",
    "files = []\n",
    "\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "\n",
    "for i in range(1,6):\n",
    "    file = f\"data_batch_{i}\"\n",
    "    file_names.append(file)\n",
    "\n",
    "file_names.append(\"test_batch\")\n",
    "\n",
    "for file in file_names:\n",
    "    cifar10_dict = unpickle(file)\n",
    "    files.append(cifar10_dict)\n",
    "\n",
    "n_of_files = len(file_names)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ee043b-da31-4147-a92b-7d37fe916073",
   "metadata": {},
   "source": [
    "## Inspect the list `files`\n",
    "* In this list i have 6 dictionaries\n",
    "* Below i get to see the keys from all the dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "72a51ec3-6f78-4e4a-8f7e-51c69eaa1973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary data_batch_1: \n",
      "Keys: dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n",
      "Batch label: b'training batch 1 of 5'\n",
      "Data size: (10000, 3072) \n",
      "\n",
      "Dictionary data_batch_2: \n",
      "Keys: dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n",
      "Batch label: b'training batch 2 of 5'\n",
      "Data size: (10000, 3072) \n",
      "\n",
      "Dictionary data_batch_3: \n",
      "Keys: dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n",
      "Batch label: b'training batch 3 of 5'\n",
      "Data size: (10000, 3072) \n",
      "\n",
      "Dictionary data_batch_4: \n",
      "Keys: dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n",
      "Batch label: b'training batch 4 of 5'\n",
      "Data size: (10000, 3072) \n",
      "\n",
      "Dictionary data_batch_5: \n",
      "Keys: dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n",
      "Batch label: b'training batch 5 of 5'\n",
      "Data size: (10000, 3072) \n",
      "\n",
      "Dictionary test_batch: \n",
      "Keys: dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n",
      "Batch label: b'testing batch 1 of 1'\n",
      "Data size: (10000, 3072) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_of_files):\n",
    "    print(f\"Dictionary {file_names[i]}: \\nKeys: {files[i].keys()}\\nBatch label: {files[i][b'batch_label']}\\nData size: {files[i][b'data'].shape} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb4dacb-03d8-4d81-ae79-17e79dc3736d",
   "metadata": {},
   "source": [
    "## Since i have the contents of the cifar10, i am keeping only the data and the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "bc00234f-7959-4681-905c-b6beb3942fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary data_batch_1: \n",
      "Keys: dict_keys([b'labels', b'data'])\n",
      "\n",
      "Dictionary data_batch_2: \n",
      "Keys: dict_keys([b'labels', b'data'])\n",
      "\n",
      "Dictionary data_batch_3: \n",
      "Keys: dict_keys([b'labels', b'data'])\n",
      "\n",
      "Dictionary data_batch_4: \n",
      "Keys: dict_keys([b'labels', b'data'])\n",
      "\n",
      "Dictionary data_batch_5: \n",
      "Keys: dict_keys([b'labels', b'data'])\n",
      "\n",
      "Dictionary test_batch: \n",
      "Keys: dict_keys([b'labels', b'data'])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_of_files):\n",
    "    #use try except because if i try to run this cell alone, an error will occur, because i have already deleted these keys\n",
    "    try:\n",
    "        del files[i][b'batch_label']\n",
    "        del files[i][b'filenames']\n",
    "    except KeyError:\n",
    "        print(\"These keys have already been deleted!\\nRun all the cells again\")\n",
    "        pass\n",
    "    print(f\"Dictionary {file_names[i]}: \\nKeys: {files[i].keys()}\\n\")\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61e9cf9-102b-47ec-8819-20841f05e802",
   "metadata": {},
   "source": [
    "## Create the X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "449cf8b3-c146-4f65-aa57-add9156f8846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape X_train: (50000, 3072)\n",
      "Shape y_train: (50000,)\n",
      "Shape X_test: (50000, 3072)\n",
      "Shape y_test: (50000,)\n",
      "\n",
      "NaN inidces in X_train: (array([], dtype=int64), array([], dtype=int64))\n",
      "NaN inidces in y_train: (array([], dtype=int64),)\n",
      "NaN inidces in X_test: (array([], dtype=int64), array([], dtype=int64))\n",
      "NaN inidces in y_test: (array([], dtype=int64),)\n"
     ]
    }
   ],
   "source": [
    "#create X_train, y_train, X-test, y_test\n",
    "X_train = np.full((50000,3072),0,dtype=int)\n",
    "X_test = np.full((10000,3072),0,dtype=int)\n",
    "y_train = np.full((50000,),0,dtype=int)\n",
    "y_test = np.full((10000,),0,dtype=int)\n",
    "\n",
    "for i in range(n_of_files):\n",
    "    if i != 5:\n",
    "        #this is the X_train, y_train\n",
    "        X_train[i*10000:(i+1)*10000,:] = files[i][b'data']\n",
    "        y_train[i*10000:(i+1)*10000] = files[i][b'labels']\n",
    "    else:\n",
    "        #i have just finished X_train, y_train\n",
    "        print(f\"Shape X_train: {X_train.shape}\\nShape y_train: {y_train.shape}\")\n",
    "        #this is the X_test, y_test\n",
    "        X_test[:,:] = files[i][b'data']\n",
    "        y_test[:] = files[i][b'labels']\n",
    "        print(f\"Shape X_test: {X_train.shape}\\nShape y_test: {y_train.shape}\\n\")\n",
    "\n",
    "#check if there is a nan value left\n",
    "print(f\"NaN inidces in X_train: {np.where(np.isnan(X_train))}\")\n",
    "print(f\"NaN inidces in y_train: {np.where(np.isnan(y_train))}\")\n",
    "print(f\"NaN inidces in X_test: {np.where(np.isnan(X_test))}\")\n",
    "print(f\"NaN inidces in y_test: {np.where(np.isnan(y_test))}\")\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deea5b4a-fedf-455f-b428-b061efbf21ea",
   "metadata": {},
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2c39f7-ebef-4a2a-b86a-b9419531f765",
   "metadata": {},
   "source": [
    "- `fit_transform` is used on `X_train` to calculate the mean and standard deviation of the training data (this is the \"fit\" part).\n",
    "It then scales `X_train` using those calculated values (this is the \"transform\" part).\n",
    "\n",
    "- The `transform` only applies the previously computed mean and standard deviation (from X_train) to scale X_test.\n",
    "This step is crucial to ensure that X_test is scaled in the same way as X_train, maintaining consistency between datasets.\n",
    "If you used `fit_transform` on X_test, it would calculate a new mean and standard deviation just for the test set, causing the training and test data to be on different scales, which would lead to unreliable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "f9da6f03-8dfc-4285-b0e4-b568b61b2b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Feature scaling\n",
    "sc_X = StandardScaler()\n",
    "\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "4cbc1a2d-f9c8-4a7c-a2d8-81f24207f733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# #!!!!!!!!!!!!!! KANO PCA KAIS TO TEST BATCH DATA ??????????? !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# pca = PCA(n_components=2000)\n",
    "# X_train = pca.fit_transform(X_train)\n",
    "# X_test = pca.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196ac4d1-e4dc-4f5d-8cbc-fd39a64e1bf8",
   "metadata": {},
   "source": [
    "## Implement KNN\n",
    "\n",
    "* Each batch file has the `data` and the `labels`\n",
    "* `Data` is a 10000(images) x 3072(rgb) matrix \n",
    "* `Labels` is a list with 10000 elements, each one corresponds to an image\n",
    "\n",
    "* So the concept is this:\n",
    "``\n",
    "I am working in the 3072 dimensional space and i have seen all the training batches.\n",
    "The new image will use the KNN to find the K nearest images, using a defined metric (Euklideian Distance,cosine,...)\n",
    "Then the majority class (label) between the K nearest images will be the class (label) of the new image \n",
    "`` \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "be8f80f3-1851-41a6-a2c0-a1acb20b7880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 1-NN: 0.4102\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the model: init 1-NN\n",
    "# !!!!!!!!!!!!!!!!!!!!!! Find out what cosine metric extually does !!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "classifier_1NN = KNeighborsClassifier(n_neighbors=1,weights=\"distance\",metric=\"cosine\")  #i noticed that with the cosine metric, the accuracy is higher than with the euclidean\n",
    "\n",
    "# Train the model\n",
    "# Only the training batches\n",
    "\n",
    "classifier_1NN.fit(X_train,y_train)\n",
    "\n",
    "# Predict the test set results\n",
    "# predict the labels from the test batch data\n",
    "\n",
    "y_pred_labels = classifier_1NN.predict(X_test)\n",
    "# y_test_labels = files[n_of_files-1][b'labels']\n",
    "\n",
    "# Evaluate the model using accuracy (y_pred_labels == y_test) / number of tests\n",
    "# number of tests = 10k\n",
    "\n",
    "print(f\"Accuracy of 1-NN: {accuracy_score(y_test,y_pred_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "06ada456-5286-4595-a6a6-7695c1e98f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 1-NN: 0.4272\n"
     ]
    }
   ],
   "source": [
    "# Define the model: init 3-NN\n",
    "classifier_3NN = KNeighborsClassifier(n_neighbors=3,weights=\"distance\",metric=\"cosine\")  #i noticed that with the cosine metric, the accuracy is higher than with the euclidean\n",
    "\n",
    "# Train the model\n",
    "# Only the training batches\n",
    "\n",
    "classifier_3NN.fit(X_train,y_train)\n",
    "\n",
    "# Predict the test set results\n",
    "# predict the labels from the test batch data\n",
    "\n",
    "y_pred_labels = classifier_3NN.predict(X_test)\n",
    "\n",
    "# Evaluate the model using accuracy (y_pred_labels == y_test_labels) / number of tests\n",
    "# number of tests = 10k\n",
    "\n",
    "print(f\"Accuracy of 1-NN: {accuracy_score(y_test_labels,y_pred_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d93090-dac3-4ede-bd79-57f89c4bac1e",
   "metadata": {},
   "source": [
    "I have tried many different things:\n",
    "* `KNeighborsClassifier(n_neighbors=k,weights=\"distance\",metric=\"cosine\")` ~ 0.35\n",
    "* `KNeighborsClassifier(n_neighbors=k,metric=\"cosine\")` ~ 0.345\n",
    "* `KNeighborsClassifier(n_neighbors=k,metric=\"euclidean\")` ~ 0.29\n",
    "* used all the other metrics: `cityblock`, `haversine`, `l1`, `l2`, `manhattan`, `nan_euclidean` < 0.35.\n",
    "* (`manhattan` = `minkowski` for p=1 and `euclidean` is `minkowski` for p = 2)\n",
    "\n",
    "* Also i used `weights = \"distance\"` and i got a slightly better accuracy. This means that neighbors that are nearer to the query point will have a greater influence on the predicted class\n",
    "The default value is `weights = \"uniform\"` (each neighbor contributes equally to the decision.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
