{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac3c449b-5bf5-47aa-b741-92e473c932b0",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "8079e5f4-6dd6-44ee-afa0-534e654693ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = []\n",
    "files = []\n",
    "\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "\n",
    "for i in range(1,6):\n",
    "    file = f\"data_batch_{i}\"\n",
    "    file_names.append(file)\n",
    "\n",
    "file_names.append(\"test_batch\")\n",
    "\n",
    "for file in file_names:\n",
    "    cifar10_dict = unpickle(file)\n",
    "    files.append(cifar10_dict)\n",
    "\n",
    "n_of_files = len(file_names)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ee043b-da31-4147-a92b-7d37fe916073",
   "metadata": {},
   "source": [
    "## Inspect the list `files`\n",
    "* In this list i have 6 dictionaries\n",
    "* Below i get to see the keys from all the dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "72a51ec3-6f78-4e4a-8f7e-51c69eaa1973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary data_batch_1: \n",
      "Keys: dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n",
      "Batch label: b'training batch 1 of 5'\n",
      "Data size: (10000, 3072) \n",
      "\n",
      "Dictionary data_batch_2: \n",
      "Keys: dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n",
      "Batch label: b'training batch 2 of 5'\n",
      "Data size: (10000, 3072) \n",
      "\n",
      "Dictionary data_batch_3: \n",
      "Keys: dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n",
      "Batch label: b'training batch 3 of 5'\n",
      "Data size: (10000, 3072) \n",
      "\n",
      "Dictionary data_batch_4: \n",
      "Keys: dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n",
      "Batch label: b'training batch 4 of 5'\n",
      "Data size: (10000, 3072) \n",
      "\n",
      "Dictionary data_batch_5: \n",
      "Keys: dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n",
      "Batch label: b'training batch 5 of 5'\n",
      "Data size: (10000, 3072) \n",
      "\n",
      "Dictionary test_batch: \n",
      "Keys: dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n",
      "Batch label: b'testing batch 1 of 1'\n",
      "Data size: (10000, 3072) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_of_files):\n",
    "    print(f\"Dictionary {file_names[i]}: \\nKeys: {files[i].keys()}\\nBatch label: {files[i][b'batch_label']}\\nData size: {files[i][b'data'].shape} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb4dacb-03d8-4d81-ae79-17e79dc3736d",
   "metadata": {},
   "source": [
    "## Since i have the contents of the cifar10, i am keeping only the data and the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "bc00234f-7959-4681-905c-b6beb3942fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary data_batch_1: \n",
      "Keys: dict_keys([b'labels', b'data'])\n",
      "\n",
      "Dictionary data_batch_2: \n",
      "Keys: dict_keys([b'labels', b'data'])\n",
      "\n",
      "Dictionary data_batch_3: \n",
      "Keys: dict_keys([b'labels', b'data'])\n",
      "\n",
      "Dictionary data_batch_4: \n",
      "Keys: dict_keys([b'labels', b'data'])\n",
      "\n",
      "Dictionary data_batch_5: \n",
      "Keys: dict_keys([b'labels', b'data'])\n",
      "\n",
      "Dictionary test_batch: \n",
      "Keys: dict_keys([b'labels', b'data'])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_of_files):\n",
    "    #use try except because if i try to run this cell alone, an error will occur, because i have already deleted these keys\n",
    "    try:\n",
    "        del files[i][b'batch_label']\n",
    "        del files[i][b'filenames']\n",
    "    except KeyError:\n",
    "        print(\"These keys have already been deleted!\\nRun all the cells again\")\n",
    "        pass\n",
    "    print(f\"Dictionary {file_names[i]}: \\nKeys: {files[i].keys()}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196ac4d1-e4dc-4f5d-8cbc-fd39a64e1bf8",
   "metadata": {},
   "source": [
    "## Implement KNN\n",
    "\n",
    "* Each batch file has the `data` and the `labels`\n",
    "* `Data` is a 10000(images) x 3072(rgb) matrix \n",
    "* `Labels` is a list with 10000 elements, each one corresponds to an image\n",
    "\n",
    "* So the concept is this:\n",
    "``\n",
    "I am working in the 3072 dimensional space and i have seen all the training batches.\n",
    "The new image will use the KNN to find the K nearest images, using a defined metric (Euklideian Distance,cosine,...)\n",
    "Then the majority class (label) between the K nearest images will be the class (label) of the new image \n",
    "`` \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "f9da6f03-8dfc-4285-b0e4-b568b61b2b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Feature scaling\n",
    "sc_X = StandardScaler()\n",
    "for i in range(n_of_files):\n",
    "    files[i][b'data'] = sc_X.fit_transform(files[i][b'data'])\n",
    "\n",
    "#!!!!!!!!!!!!!! KANO PCA KAIS TO TEST BATCH DATA ??????????? !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# pca = PCA(n_components=2)\n",
    "# for i in range(n_of_files):\n",
    "#     files[i][b'data'] = pca.fit_transform((files[i][b'data']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "be8f80f3-1851-41a6-a2c0-a1acb20b7880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 1-NN: 0.1318\n"
     ]
    }
   ],
   "source": [
    "# Define the model: init 1-NN\n",
    "k=1\n",
    "# !!!!!!!!!!!!!!!!!!!!!! Find out what cosine metric extually does !!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "classifier_1NN = KNeighborsClassifier(n_neighbors=k,weights=\"distance\",metric=\"cosine\")  #i noticed that with the cosine metric, the accuracy is higher than with the euclidean\n",
    "\n",
    "# Train the model\n",
    "# Only the training batches\n",
    "\n",
    "for i in range(n_of_files-1):\n",
    "    classifier_1NN.fit(files[i][b'data'],files[i][b'labels'])\n",
    "\n",
    "# Predict the test set results\n",
    "# predict the labels from the test batch data\n",
    "\n",
    "y_pred_labels = classifier_1NN.predict(files[n_of_files-1][b'data'])\n",
    "y_test_labels = files[n_of_files-1][b'labels']\n",
    "\n",
    "# Evaluate the model using accuracy (y_pred_labels == y_test_labels) / number of tests\n",
    "# number of tests = 10k\n",
    "\n",
    "print(f\"Accuracy of {k}-NN: {accuracy_score(y_test_labels,y_pred_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "06ada456-5286-4595-a6a6-7695c1e98f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 3-NN: 0.1334\n"
     ]
    }
   ],
   "source": [
    "# Define the model: init 3-NN\n",
    "k=3\n",
    "classifier_3NN = KNeighborsClassifier(n_neighbors=k,weights=\"distance\",metric=\"cosine\")  #i noticed that with the cosine metric, the accuracy is higher than with the euclidean\n",
    "\n",
    "# Train the model\n",
    "# Only the training batches\n",
    "\n",
    "for i in range(n_of_files-1):\n",
    "    classifier_3NN.fit(files[i][b'data'],files[i][b'labels'])\n",
    "\n",
    "\n",
    "# Predict the test set results\n",
    "# predict the labels from the test batch data\n",
    "\n",
    "y_pred_labels = classifier_3NN.predict(files[n_of_files-1][b'data'])\n",
    "y_test_labels = files[n_of_files-1][b'labels']\n",
    "\n",
    "# Evaluate the model using accuracy (y_pred_labels == y_test_labels) / number of tests\n",
    "# number of tests = 10k\n",
    "\n",
    "print(f\"Accuracy of {k}-NN: {accuracy_score(y_test_labels,y_pred_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d93090-dac3-4ede-bd79-57f89c4bac1e",
   "metadata": {},
   "source": [
    "I have tried many different things:\n",
    "* `KNeighborsClassifier(n_neighbors=k,weights=\"distance\",metric=\"cosine\")` ~ 0.35\n",
    "* `KNeighborsClassifier(n_neighbors=k,metric=\"cosine\")` ~ 0.345\n",
    "* `KNeighborsClassifier(n_neighbors=k,metric=\"euclidean\")` ~ 0.29\n",
    "* used all the other metrics: `cityblock`, `haversine`, `l1`, `l2`, `manhattan`, `nan_euclidean` < 0.35.\n",
    "* (`manhattan` = `minkowski` for p=1 and `euclidean` is `minkowski` for p = 2)\n",
    "\n",
    "* Also i used `weights = \"distance\"` and i got a slightly better accuracy. This means that neighbors that are nearer to the query point will have a greater influence on the predicted class\n",
    "The default value is `weights = \"uniform\"` (each neighbor contributes equally to the decision.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
