{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "043520cc-ceb0-4492-97ea-857e3d22f0af",
   "metadata": {},
   "source": [
    "# Report ενδιάμεσης εργασίας\n",
    "\n",
    "In this notebook i will be comparing the performance of the 1NN, 3NN and nearest centroid classifier, using the `CIFAR-10` dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3c449b-5bf5-47aa-b741-92e473c932b0",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "- I am importing the `numpy` library because the `unpickle()` function returns numpy arrays and i will need to manipulate them\n",
    "\n",
    "- I used the recommended function `unpickle(file)` that i saw in this site https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "\n",
    "- In this cell, i am storing the batch names in an array called `file_names` and i am also using an array called `files` to store the 6 dictionaries that i will get from the `unpickle()` function.\n",
    "\n",
    "- At the end i am printing the number of the files that are in `files`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8079e5f4-6dd6-44ee-afa0-534e654693ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "file_names = []\n",
    "files = []\n",
    "\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "\n",
    "for i in range(1,6):\n",
    "    file = f\"data_batch_{i}\"\n",
    "    file_names.append(file)\n",
    "\n",
    "file_names.append(\"test_batch\")\n",
    "\n",
    "for file in file_names:\n",
    "    cifar10_dict = unpickle(file)\n",
    "    files.append(cifar10_dict)\n",
    "\n",
    "n_of_files = len(file_names)\n",
    "n_of_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ee043b-da31-4147-a92b-7d37fe916073",
   "metadata": {},
   "source": [
    "## Inspect the list `files`\n",
    "* In this list i have 6 dictionaries\n",
    "* Below i get to see the keys from all the dictionaries\n",
    "* I notice that i have 5 training batches, each one with 50000 images and 1 testing batch with 10000 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72a51ec3-6f78-4e4a-8f7e-51c69eaa1973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary data_batch_1: \n",
      "Keys: dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n",
      "Batch label: b'training batch 1 of 5'\n",
      "Data size: (10000, 3072) \n",
      "\n",
      "Dictionary data_batch_2: \n",
      "Keys: dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n",
      "Batch label: b'training batch 2 of 5'\n",
      "Data size: (10000, 3072) \n",
      "\n",
      "Dictionary data_batch_3: \n",
      "Keys: dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n",
      "Batch label: b'training batch 3 of 5'\n",
      "Data size: (10000, 3072) \n",
      "\n",
      "Dictionary data_batch_4: \n",
      "Keys: dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n",
      "Batch label: b'training batch 4 of 5'\n",
      "Data size: (10000, 3072) \n",
      "\n",
      "Dictionary data_batch_5: \n",
      "Keys: dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n",
      "Batch label: b'training batch 5 of 5'\n",
      "Data size: (10000, 3072) \n",
      "\n",
      "Dictionary test_batch: \n",
      "Keys: dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n",
      "Batch label: b'testing batch 1 of 1'\n",
      "Data size: (10000, 3072) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_of_files):\n",
    "    print(f\"Dictionary {file_names[i]}: \\nKeys: {files[i].keys()}\\nBatch label: {files[i][b'batch_label']}\\nData size: {files[i][b'data'].shape} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb4dacb-03d8-4d81-ae79-17e79dc3736d",
   "metadata": {},
   "source": [
    "## Since i have the contents of the cifar10, i am keeping only the data and the labels\n",
    "\n",
    "- I am printing the name and the keys of each one of the 6 dictionaries, in order to be sure, that i am only keeping the data and the labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc00234f-7959-4681-905c-b6beb3942fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary data_batch_1: \n",
      "Keys: dict_keys([b'labels', b'data'])\n",
      "\n",
      "Dictionary data_batch_2: \n",
      "Keys: dict_keys([b'labels', b'data'])\n",
      "\n",
      "Dictionary data_batch_3: \n",
      "Keys: dict_keys([b'labels', b'data'])\n",
      "\n",
      "Dictionary data_batch_4: \n",
      "Keys: dict_keys([b'labels', b'data'])\n",
      "\n",
      "Dictionary data_batch_5: \n",
      "Keys: dict_keys([b'labels', b'data'])\n",
      "\n",
      "Dictionary test_batch: \n",
      "Keys: dict_keys([b'labels', b'data'])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_of_files):\n",
    "    #use try except because if i try to run this cell alone, an error will occur, because i have already deleted these keys\n",
    "    try:\n",
    "        del files[i][b'batch_label']\n",
    "        del files[i][b'filenames']\n",
    "    except KeyError:\n",
    "        print(\"These keys have already been deleted!\\nRun all the cells again\")\n",
    "        pass\n",
    "    print(f\"Dictionary {file_names[i]}: \\nKeys: {files[i].keys()}\\n\")\n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61e9cf9-102b-47ec-8819-20841f05e802",
   "metadata": {},
   "source": [
    "## Create the X_train, y_train, X_test, y_test\n",
    "\n",
    "* At first, i initialize with zeroes 4 arrays.\n",
    "* Then the `X_train` and the `y_train` are filled. The first with the 50000*3072 data (50000 images) from each batch file and the second with the 50000 labels from each batch file.\n",
    "* When i get to the testing batch, indexed at the 5th place in the `files` array, i fill the `X_test` with 10000*3072 data (10000 images) and the `y_test` with the 10000 labels\n",
    "* I am printing the size of each array, to showcase that the sizes are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "449cf8b3-c146-4f65-aa57-add9156f8846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape X_train: (50000, 3072)\n",
      "Shape y_train: (50000,)\n",
      "Shape X_test: (10000, 3072)\n",
      "Shape y_test: (10000,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create X_train, y_train, X_test, y_test\n",
    "X_train = np.full((50000,3072),0,dtype=int)\n",
    "X_test = np.full((10000,3072),0,dtype=int)\n",
    "y_train = np.full((50000,),0,dtype=int)\n",
    "y_test = np.full((10000,),0,dtype=int)\n",
    "\n",
    "for i in range(n_of_files):\n",
    "    if i != 5:\n",
    "        #this is the X_train, y_train\n",
    "        X_train[i*10000:(i+1)*10000,:] = files[i][b'data']\n",
    "        y_train[i*10000:(i+1)*10000] = files[i][b'labels']\n",
    "    else:\n",
    "        #i have just finished X_train, y_train\n",
    "        print(f\"Shape X_train: {X_train.shape}\\nShape y_train: {y_train.shape}\")\n",
    "        #this is the X_test, y_test\n",
    "        X_test[:,:] = files[i][b'data']\n",
    "        y_test[:] = files[i][b'labels']\n",
    "        print(f\"Shape X_test: {X_test.shape}\\nShape y_test: {y_test.shape}\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9058a930-7390-47e6-88f0-9bc77e20d3a5",
   "metadata": {},
   "source": [
    "## Preprocessing the data\n",
    "\n",
    "In the cell bellow, i will try some methods to improve the accuracy of my algorithms by preprocessing the data\n",
    "\n",
    "- `fit_transform` is used on `X_train` to calculate the mean and standard deviation of the training data (this is the \"fit\" part).\n",
    "It then scales `X_train` using those calculated values (this is the \"transform\" part).\n",
    "\n",
    "- The `transform` only applies the previously computed mean and standard deviation (from X_train) to scale X_test.\n",
    "\n",
    "- Using the PCA, the dimensionality of the dataset is reduced to 100 features (these are the features with the most variance), by excluding the features with the least variance\n",
    "\n",
    "- Adding some guassian noise an the train data improves the accuracy and the robustness of our model. In more detail, the noise that i am adding has mean = 0 and varaince = 2. With bigger values, i am getting worse accuracy. This way my training set gets twice as big as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "558e39c7-fc67-49e9-a6e8-84ea2fd58ba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 100000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Feature scaling\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "\n",
    "\n",
    "#PCA\n",
    "pca = PCA(n_components=100)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "\n",
    "#adding gaussian noise to the dataset\n",
    "noise_factor = 0.1  # Adjust the noise level as needed\n",
    "X_train_noisy = X_train + noise_factor * np.random.normal(loc=0.0, scale=2.0, size=X_train.shape)\n",
    "\n",
    "\n",
    "X_train_combined = np.concatenate((X_train,X_train_noisy),axis=0)\n",
    "y_train_combined = np.concatenate((y_train,y_train),axis=0)   #i am putting the same array twice\n",
    "\n",
    "len(X_train_combined), len(y_train_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e141f0-0a6a-4ca5-a45c-09240fac5454",
   "metadata": {},
   "source": [
    "## Turn train and test data into tensors\n",
    "\n",
    "I am doing this, because i might try to use my GPU for acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc731e87-2806-4d8f-b023-0b1d2052ae27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-22.0557,  12.2849],\n",
       "         [  4.0135,  -5.0492],\n",
       "         [ 21.1123, -47.6872],\n",
       "         [-39.2313,   2.3340],\n",
       "         [-15.5716, -16.6879]]),\n",
       " tensor([6., 9., 9., 4., 1.]),\n",
       " tensor([3., 8., 8., 0., 6.]),\n",
       " tensor([[-11.3506,   3.4136],\n",
       "         [ 33.1129, -42.5072],\n",
       "         [ 11.9410, -35.2011],\n",
       "         [ 30.2431, -18.6372],\n",
       "         [-16.7043,  18.7802]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "X_train_combined = torch.from_numpy(X_train_combined).type(torch.float)  #important to define the default type for tensors\n",
    "y_train_combined = torch.from_numpy(y_train_combined).type(torch.float)\n",
    "X_test = torch.from_numpy(X_test).type(torch.float)\n",
    "y_test = torch.from_numpy(y_test).type(torch.float)\n",
    "\n",
    "X_train_combined[:5,:2], y_train_combined[:5], y_test[:5], X_test[:5,:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196ac4d1-e4dc-4f5d-8cbc-fd39a64e1bf8",
   "metadata": {},
   "source": [
    "## Implement 1NN\n",
    "\n",
    "* So the concept is this:\n",
    "``\n",
    "I am working in the 3072 dimensional space and i have seen all the training batches.\n",
    "The new image will use the KNN to find the K nearest images, using a defined metric (Euklideian Distance,cosine,...)\n",
    "Then the majority class (label) between the K nearest images will be the class (label) of the new image \n",
    "``\n",
    "\n",
    "* I noticed that by using the `weight = \"distance\"` and `metric = \"cosine\"` the accuracy improves. I noticed that by trying all the `weights` and the `metrics` available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be8f80f3-1851-41a6-a2c0-a1acb20b7880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 1-NN: 0.4239\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define the model: init 1-NN\n",
    "classifier_1NN = KNeighborsClassifier(n_neighbors=1,weights=\"distance\",metric=\"cosine\")  #i noticed that with the cosine metric, the accuracy is higher than with the euclidean\n",
    "\n",
    "# Train the model\n",
    "# Only the training batches\n",
    "classifier_1NN.fit(X_train_combined,y_train_combined)\n",
    "\n",
    "# Predict the test set results\n",
    "# predict the labels from the test batch data\n",
    "y_pred_labels = classifier_1NN.predict(X_test)\n",
    "\n",
    "# Evaluate the model using accuracy (y_pred_labels == y_test) / number of tests\n",
    "# number of tests = 10k\n",
    "print(f\"Accuracy of 1-NN: {accuracy_score(y_test,y_pred_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b0979b-f2d0-4657-8a5f-f44a71701e20",
   "metadata": {},
   "source": [
    "## Implement the 3NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06ada456-5286-4595-a6a6-7695c1e98f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 3-NN: 0.4253\n"
     ]
    }
   ],
   "source": [
    "# Define the model: init 3-NN\n",
    "classifier_3NN = KNeighborsClassifier(n_neighbors=3,weights=\"distance\",metric=\"cosine\")  #i noticed that with the cosine metric, the accuracy is higher than with the euclidean\n",
    "\n",
    "# Train the model\n",
    "# Only the training batches\n",
    "classifier_3NN.fit(X_train_combined,y_train_combined)\n",
    "\n",
    "# Predict the test set results\n",
    "# predict the labels from the test batch data\n",
    "y_pred_labels = classifier_3NN.predict(X_test)\n",
    "\n",
    "# Evaluate the model using accuracy (y_pred_labels == y_test_labels) / number of tests\n",
    "# number of tests = 10k\n",
    "print(f\"Accuracy of 3-NN: {accuracy_score(y_test,y_pred_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d93090-dac3-4ede-bd79-57f89c4bac1e",
   "metadata": {},
   "source": [
    "I have tried many different things:\n",
    "* used all the other metrics: `cityblock`, `haversine`, `l1`, `l2`, `manhattan`, `nan_euclidean` < 0.35.\n",
    "\n",
    "* Also i used `weights = \"distance\"` and i got a slightly better accuracy. This means that neighbors that are nearer to the query point will have a greater influence on the predicted class\n",
    "The default value is `weights = \"uniform\"` (each neighbor contributes equally to the decision.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e29bd4f-2c65-40ea-af1c-f03742452c8c",
   "metadata": {},
   "source": [
    "## Implement of Nearest Centroid classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71cfc6b5-aeca-47d2-8888-e3f7ec037a91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.277"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "classifier_KNC = NearestCentroid(shrink_threshold=1.4)\n",
    "\n",
    "#training the classifier\n",
    "classifier_KNC.fit(X_train_combined,y_train_combined)\n",
    "\n",
    "y_preds_labels = classifier_KNC.predict(X_test)\n",
    "\n",
    "accuracy_score(y_test,y_preds_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
